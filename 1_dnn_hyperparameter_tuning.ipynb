{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G91h9KUnFbhm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import json\n",
        "from typing import Tuple, List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAEIwTbBFbhr"
      },
      "outputs": [],
      "source": [
        "%pip install wandb\n",
        "import wandb\n",
        "\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_GXyRiQFbht"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "torch.backends.cudnn.determinstic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "torch.cuda.set_device(0)\n",
        "device = torch.device(\"cuda\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pL1801_wWQLt"
      },
      "outputs": [],
      "source": [
        "!gdown 1AuCGKE9nzNjGfvo4A5CX-JdkvWPfivUV # download dataframe.pkl\n",
        "\n",
        "columns_to_drop = [\"song_id\", \" valence_std\", \" arousal_std\"]\n",
        "test_fraction = 0.2\n",
        "random_state = 200\n",
        "\n",
        "data =pd.read_pickle(\"/content/dataframe.pkl\")\n",
        "data = data.drop(columns=columns_to_drop)\n",
        "data[data.columns[[0, 1]]] = data[data.columns[[0, 1]]].div(10)  # scale labels to [0.1-0.9]\n",
        "\n",
        "test = data.sample(frac=test_fraction, random_state=random_state)\n",
        "train = data.drop(test.index)\n",
        "\n",
        "features = train[train.columns[2:]]  # normalize all, except labels\n",
        "train[train.columns[2:]] = (features - features.mean()) / features.std()\n",
        "\n",
        "test[test.columns[2:]] = (test[test.columns[2:]] - features.mean()) / features.std()\n",
        "\n",
        "train_data = train[train.columns[2:]]\n",
        "train_labels = train[train.columns[:2]]  # valence, arousal\n",
        "test_data = test[test.columns[2:]]\n",
        "test_labels = test[test.columns[:2]]\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrMoDlDoFbh2"
      },
      "outputs": [],
      "source": [
        "!gdown 1rD8kwaVtWv1jihqeaP9FnqpE61ZTmoXt # download config\n",
        "sweep_config = json.load(open(\"/content/dnn_wandb_config.json\"))\n",
        "sweep_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7by4ENjFbh5"
      },
      "outputs": [],
      "source": [
        "def build_dataset(batch_size: int) -> Tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader]:\n",
        "    train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(train_data.values.astype(float)), torch.from_numpy(train_labels.values.astype(float)))\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "    \n",
        "    test_dataset = torch.utils.data.TensorDataset(torch.from_numpy(test_data.values.astype(float)), torch.from_numpy(test_labels.values.astype(float)))\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "def build_layer(input_size: int, output_size: int, activation: str, dropout: float, batch_norm: bool) -> List[nn.Module]:\n",
        "    \n",
        "    modules = [nn.Linear(input_size, output_size, bias=not batch_norm)]\n",
        "\n",
        "    if activation == \"relu\":\n",
        "        activation = nn.ReLU()\n",
        "    elif activation == \"tanh\":\n",
        "        activation = nn.Tanh()\n",
        "    elif activation == \"leakyrelu\":\n",
        "        activation = nn.LeakyReLU()\n",
        "    elif activation == \"sigmoid\":\n",
        "        activation = nn.Sigmoid()\n",
        "    modules.append(activation)\n",
        "    \n",
        "    if dropout != 0.0:\n",
        "        modules.append(nn.Dropout(p=dropout))\n",
        "        \n",
        "    if batch_norm:\n",
        "        modules.append(nn.BatchNorm1d(output_size))\n",
        "        \n",
        "    return modules\n",
        "\n",
        "def build_network(input_size: int, output_size: int, mult: float, max_hidden: int, layers_amount: int, activation: str, dropout: float, batch_norm: bool) -> nn.Sequential:\n",
        "    modules = []\n",
        "    in_size = input_size\n",
        "    for i in range(layers_amount-1):\n",
        "        if mult > 5: # mult > 5 means that we want all layers to have same, given size\n",
        "            out_size = mult\n",
        "        else: # otherwise if number of layer i < half the amount of layers, then we want next layer to have this_layer_neurons_amount * mult   else    / mult\n",
        "            if i < layers_amount/2:\n",
        "                out_size = in_size*mult\n",
        "            else:\n",
        "                out_size = in_size/mult\n",
        "            out_size = min(out_size, max_hidden)\n",
        "\n",
        "        layer = build_layer(int(in_size), int(out_size), activation, dropout, batch_norm)\n",
        "\n",
        "        for module in layer:\n",
        "            modules.append(module)\n",
        "        \n",
        "        in_size = out_size\n",
        "    \n",
        "    modules.append(nn.Linear(int(in_size), output_size, bias=not batch_norm)) # add output layer\n",
        "\n",
        "    network = nn.Sequential(*modules)\n",
        "    \n",
        "    return network.to(device)\n",
        "        \n",
        "\n",
        "def build_optimizer(network: nn.Sequential, optimizer: str, learning_rate: float) -> optim.Optimizer:\n",
        "    if optimizer == \"adam\":\n",
        "        optimizer = optim.Adam(network.parameters(), lr=learning_rate)\n",
        "    elif optimizer == \"adamw\":\n",
        "        optimizer = optim.AdamW(network.parameters(), lr=learning_rate)\n",
        "    return optimizer\n",
        "\n",
        "def build_loss_func(loss_func: str) -> nn.Module:\n",
        "    if loss_func == \"mse\":\n",
        "        loss_func = nn.MSELoss()\n",
        "    elif loss_func == \"huber\":\n",
        "        loss_func = nn.HuberLoss()\n",
        "    elif loss_func == \"l1\":\n",
        "        loss_func = nn.L1Loss()\n",
        "    return loss_func\n",
        "\n",
        "def train_epoch(model: nn.Sequential, loader: torch.utils.data.DataLoader, optimizer: optim.Optimizer, loss_func: nn.Module) -> float:\n",
        "    model.train()\n",
        "    cumu_loss = 0\n",
        "    for data, target in loader:\n",
        "        data, target = data.float().to(device), target.float().to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss = loss_func(model(data), target)\n",
        "        cumu_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        wandb.log({\"batch loss\": loss.item()})\n",
        "\n",
        "    return cumu_loss / len(loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def calculate_metric(max_diff_arr: List[float], loader: torch.utils.data.DataLoader, model: nn.Sequential) -> List[float]:\n",
        "    model.eval()\n",
        "    res = []\n",
        "    with torch.no_grad():\n",
        "        for max_diff in max_diff_arr:\n",
        "            num_preds = 0.\n",
        "            true_preds = 0.\n",
        "            for data_inputs, data_labels in loader:\n",
        "                data_inputs, data_labels = data_inputs.to(device), data_labels.to(device)\n",
        "                preds = model(data_inputs.float())\n",
        "                preds = preds.cpu().detach().numpy()\n",
        "                labels = data_labels.cpu().detach().numpy()\n",
        "                for i in range(len(preds)):\n",
        "                    if (np.abs(preds[i][0] - labels[i][0]) <= max_diff) and (np.abs(preds[i][1] - labels[i][1]) <= max_diff):\n",
        "                        true_preds += 1\n",
        "                    num_preds+=1\n",
        "            res.append(true_preds/num_preds)\n",
        "    return res\n",
        "\n",
        "def train(config=None) -> None:\n",
        "    # Initialize a new wandb run\n",
        "    with wandb.init(config=config):\n",
        "    # If called by wandb.agent, as below,\n",
        "    # this config will be set by Sweep Controller\n",
        "        config = wandb.config\n",
        "\n",
        "        train_loader, test_loader = build_dataset(config.batch_size)\n",
        "        network = build_network(config.input_size, config.output_size, config.mult, config.max_hidden,\n",
        "                                config.layers_amount, config.activation, config.dropout, config.batch_norm)\n",
        "        optimizer = build_optimizer(network, config.optimizer, config.learning_rate)\n",
        "        loss_func = build_loss_func(config.loss_func)\n",
        "\n",
        "        best_acc = 0 #best accuracy for 0.05 max diff\n",
        "        max_diffs = [0.01, 0.03, 0.05]\n",
        "        for epoch in range(config.epochs):\n",
        "            avg_loss = train_epoch(network, train_loader, optimizer, loss_func)\n",
        "            wandb.log({\"loss\": avg_loss, \"epoch\": epoch})\n",
        "\n",
        "            metric_eval = calculate_metric(max_diffs, test_loader, network)\n",
        "            if metric_eval[2] > best_acc:\n",
        "                best_acc = metric_eval[2]\n",
        "            for max_diff, acc in zip(max_diffs, metric_eval):\n",
        "                wandb.log({f\"acc_{max_diff}_eval\": acc})\n",
        "\n",
        "            metric_train = calculate_metric(max_diffs, train_loader, network)\n",
        "            for max_diff, acc in zip(max_diffs, metric_train):\n",
        "                wandb.log({f\"acc_{max_diff}_train\": acc})\n",
        "        wandb.log({f\"best_accuracy_eval\": best_acc})\n"
      ],
      "metadata": {
        "id": "GSfNjVAGaUr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep=sweep_config, entity = \"rozpoznawanie_emocji_olejnik\", project=\"dnn_refactor_5\")"
      ],
      "metadata": {
        "id": "kh6TapORfgid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxHdr75uFbh9",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "wandb.agent(sweep_id, train, count=25)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}